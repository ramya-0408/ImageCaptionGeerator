{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting caption.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile caption.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import tensorflow as tf\n",
    "from pickle import load\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "\n",
    "def greedySearch(photo, max_length=34):\n",
    "    global model, wordtoix, ixtoword\n",
    "    in_text = 'startseq'\n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "\n",
    "def get_predictions():\n",
    "    global images, encoding_test\n",
    "    index = np.random.choice(1000)\n",
    "    pic = list(encoding_test.keys())[index]\n",
    "    image = encoding_test[pic].reshape((1, 2048))\n",
    "    x = plt.imread(images+pic)\n",
    "    # np.resize(x, (-1, 200, 350))\n",
    "    st.sidebar.image(x, use_column_width=True)\n",
    "    caption = greedySearch(image)\n",
    "    return caption\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='This app predicts image captions.')\n",
    "    parser.add_argument('--modelfile',\n",
    "                        help='File Path for trained model text.')\n",
    "    parser.add_argument('--textdir',\n",
    "                        help='File Path for Flickr dataset text.')\n",
    "    parser.add_argument('--picklefile',\n",
    "                        help='File Path for encoded pickle.')\n",
    "    parser.add_argument('--imagedir',\n",
    "                        help='Root directory path for Flickr dataset images.')\n",
    "    parser.add_argument('--descriptions',\n",
    "                        help='File Path for descriptions.')\n",
    "    try:\n",
    "        args = parser.parse_args()\n",
    "    except SystemExit as e:\n",
    "        os._exit(e.code)\n",
    "\n",
    "    filename = args.textdir\n",
    "    train = load_set(filename)\n",
    "\n",
    "    # descriptions\n",
    "    train_descriptions = load_clean_descriptions(args.descriptions, train)\n",
    "\n",
    "    all_train_captions = []\n",
    "    for key, val in train_descriptions.items():\n",
    "        for cap in val:\n",
    "            all_train_captions.append(cap)\n",
    "\n",
    "    word_count_threshold = 10\n",
    "    word_counts = dict()\n",
    "    ixtoword = dict()\n",
    "    wordtoix = dict()\n",
    "    nsents = 0\n",
    "    ix = 1\n",
    "\n",
    "    for sent in all_train_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "\n",
    "    model = tf.keras.models.load_model(args.modelfile)\n",
    "\n",
    "    images = args.imagedir\n",
    "\n",
    "    with open(args.picklefile, \"rb\") as encoded_pickle:\n",
    "        encoding_test = load(encoded_pickle)\n",
    "\n",
    "    st.title('Image caption generator')\n",
    "    st.sidebar.markdown('## Input Image')\n",
    "\n",
    "    if st.button('Generate a Random Image'):\n",
    "        caption = get_predictions()\n",
    "    else:\n",
    "        caption = ''\n",
    "    st.sidebar.markdown('## Caption Generated is: ')\n",
    "    st.sidebar.markdown(caption)\n",
    "main_modified.py\n",
    "Displaying main_modified.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
