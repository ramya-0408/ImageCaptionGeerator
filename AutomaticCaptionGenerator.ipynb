{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting caption.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile caption.py\n",
    "\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "from pickle import load\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_length = 34\n",
    "\n",
    "\n",
    "\n",
    "def load_doc(filename):\n",
    "\tfile = open(filename, 'r')\n",
    "\ttext = file.read()\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "\n",
    "filename = \"C:/Users/pc/Downloads/Flickr Dataset/Flickr dataset/Flickr8k.token.txt\"\n",
    "doc = load_doc(filename)\n",
    "\n",
    "\n",
    "def load_descriptions(doc):\n",
    "\tmapping = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\ttokens = line.split()\n",
    "\t\tif len(line) < 2:\n",
    "\t\t\tcontinue\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\timage_id = image_id.split('.')[0]\n",
    "\t\timage_desc = ' '.join(image_desc)\n",
    "\t\tif image_id not in mapping:\n",
    "\t\t\tmapping[image_id] = list()\n",
    "\t\tmapping[image_id].append(image_desc)\n",
    "\treturn mapping\n",
    "\n",
    "\n",
    "\n",
    "descriptions = load_descriptions(doc)\n",
    "\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor key, desc_list in descriptions.items():\n",
    "\t\tfor i in range(len(desc_list)):\n",
    "\t\t\tdesc = desc_list[i]\n",
    "\t\t\tdesc = desc.split()\n",
    "\t\t\tdesc = [word.lower() for word in desc]\n",
    "\t\t\tdesc = [w.translate(table) for w in desc]\n",
    "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
    "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
    "\t\t\tdesc_list[i] = ' '.join(desc)\n",
    "\n",
    "\n",
    "\n",
    "clean_descriptions(descriptions)\n",
    "\n",
    "\n",
    "def load_set(filename):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdataset = list()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\tif len(line) < 1:\n",
    "\t\t\tcontinue\n",
    "\t\tidentifier = line.split('.')[0]\n",
    "\t\tdataset.append(identifier)\n",
    "\treturn set(dataset)\n",
    "\n",
    "\n",
    "filename = 'C:/Users/pc/Downloads/Flickr Dataset/Flickr dataset/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "\n",
    "\n",
    "\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "\tdoc = load_doc(filename)\n",
    "\tdescriptions = dict()\n",
    "\tfor line in doc.split('\\n'):\n",
    "\t\ttokens = line.split()\n",
    "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
    "\t\tif image_id in dataset:\n",
    "\t\t\tif image_id not in descriptions:\n",
    "\t\t\t\tdescriptions[image_id] = list()\n",
    "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "\t\t\tdescriptions[image_id].append(desc)\n",
    "\treturn descriptions\n",
    "\n",
    "\n",
    "\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "\n",
    "all_train_captions = []\n",
    "for key, val in train_descriptions.items():\n",
    "\tfor cap in val:\n",
    "\t\tall_train_captions.append(cap)\n",
    "\n",
    "word_count_threshold = 10\n",
    "word_counts = {}\n",
    "nsents = 0\n",
    "for sent in all_train_captions:\n",
    "\tnsents += 1\n",
    "\tfor w in sent.split(' '):\n",
    "\t\tword_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "\n",
    "ixtoword = {}\n",
    "wordtoix = {}\n",
    "\n",
    "ix = 1\n",
    "for w in vocab:\n",
    "\twordtoix[w] = ix\n",
    "\tixtoword[ix] = w\n",
    "\tix += 1\n",
    "\n",
    "\n",
    "def greedySearch(photo, model):\n",
    "    in_text = 'startseq'        \n",
    "    for i in range(max_length):\n",
    "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        photo = np.resize(photo, (1,2048))\n",
    "        yhat = model.predict([photo,sequence], verbose=0)        \n",
    "        yhat = np.argmax(yhat)\n",
    "        word = ixtoword[yhat]\n",
    "        in_text += ' ' + word\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    final = in_text.split()\n",
    "    final = final[1:-1]\n",
    "    final = ' '.join(final)\n",
    "    return final\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%%writefile caption.py\n",
    "\n",
    "\n",
    "\n",
    "def get_predictions():\n",
    "    model = tf.keras.models.load_model('C:/Users/pc/Desktop/captiongenerator/model_4.h5')\n",
    "\n",
    "\n",
    "    images = 'C:/Users/pc/Downloads/Flickr Dataset/Flicker8k_Dataset_Image/'\n",
    "    with open(\"C:/Users/pc/Downloads/Flickr Dataset/Flickr dataset/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
    "        encoding_test = load(encoded_pickle)\n",
    "    index = np.random.choice(1000)\n",
    "    pic = list(encoding_test.keys())[index] \n",
    "    image = encoding_test[pic].reshape((1,2048))\n",
    "    x=plt.imread(images+pic)\n",
    "    st.sidebar.image(x,use_column_width=True)  \n",
    "    caption=greedySearch(image,model)\n",
    "    st.write('## Caption Generated is: ')\n",
    "    st.write(caption)\n",
    "\n",
    "st.title('Image caption generator')\n",
    "\n",
    "\n",
    "st.sidebar.markdown('## Input Image')\n",
    "if st.button('Generate a Random Image'):\n",
    "    get_predictions()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
